{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59abca67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ID                                              tweet  \\\n",
      "0  1296010336907038720t  @cath__kath AstraZeneca is made with the kidne...   \n",
      "1  1336808189677940736t  It begins. Please find safe alternatives to th...   \n",
      "2  1329488407307956231t  @PaolaQP1231 Well, I mean congratulations Covi...   \n",
      "3  1364194604459900934t  @BorisJohnson for those of us that do not wish...   \n",
      "4  1375938799247765515t  She has been trying to speak out: writing lett...   \n",
      "\n",
      "               labels  \n",
      "0         ingredients  \n",
      "1         side-effect  \n",
      "2         side-effect  \n",
      "3           mandatory  \n",
      "4  side-effect rushed  \n",
      "labels\n",
      "side-effect    3805\n",
      "ineffective    1672\n",
      "rushed         1477\n",
      "pharma         1273\n",
      "mandatory       783\n",
      "unnecessary     722\n",
      "none            629\n",
      "political       626\n",
      "conspiracy      487\n",
      "ingredients     436\n",
      "country         201\n",
      "religious        64\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('mLabel_tweets.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Check the distribution of labels\n",
    "label_counts = df['labels'].apply(lambda x: x.split()).explode().value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458279b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saqeeb\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the tweet text\n",
    "tokens = tokenizer.batch_encode_plus(\n",
    "    df['tweet'].tolist(),\n",
    "    max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f514a0a-21ea-4650-bd46-0d1d07a24ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Split the labels by space and convert to list\n",
    "df['labels'] = df['labels'].apply(lambda x: x.split())\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(df['labels'])\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0894d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\saqeeb\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=12,  # Number of labels in your dataset\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e221115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset = TensorDataset(tokens['input_ids'], attention_masks, labels)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476cad1c-14bb-4153-8db6-7cba0e72827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([32, 12])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    b_input_ids, b_attention_mask, b_labels = batch\n",
    "    outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "    print(f\"Logits shape: {outputs.logits.shape}\")\n",
    "    break  # Print shapes for the first batch only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa7fdff-3eb7-43ed-9bb1-52a2affbfc2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Step 0/248, Loss: 0.7129769921302795\n",
      "Epoch 1/1, Step 10/248, Loss: 0.5116710662841797\n",
      "Epoch 1/1, Step 20/248, Loss: 0.44817209243774414\n",
      "Epoch 1/1, Step 30/248, Loss: 0.38861727714538574\n",
      "Epoch 1/1, Step 40/248, Loss: 0.3531745374202728\n",
      "Epoch 1/1, Step 50/248, Loss: 0.356967955827713\n",
      "Epoch 1/1, Step 60/248, Loss: 0.3099426329135895\n",
      "Epoch 1/1, Step 70/248, Loss: 0.32017675042152405\n",
      "Epoch 1/1, Step 80/248, Loss: 0.30607494711875916\n",
      "Epoch 1/1, Step 90/248, Loss: 0.27705344557762146\n",
      "Epoch 1/1, Step 100/248, Loss: 0.3017367422580719\n",
      "Epoch 1/1, Step 110/248, Loss: 0.26648417115211487\n",
      "Epoch 1/1, Step 120/248, Loss: 0.2903866767883301\n",
      "Epoch 1/1, Step 130/248, Loss: 0.24678438901901245\n",
      "Epoch 1/1, Step 140/248, Loss: 0.2542618215084076\n",
      "Epoch 1/1, Step 150/248, Loss: 0.2980482876300812\n",
      "Epoch 1/1, Step 160/248, Loss: 0.29154321551322937\n",
      "Epoch 1/1, Step 170/248, Loss: 0.28666046261787415\n",
      "Epoch 1/1, Step 180/248, Loss: 0.27955004572868347\n",
      "Epoch 1/1, Step 190/248, Loss: 0.290488600730896\n",
      "Epoch 1/1, Step 200/248, Loss: 0.2807404696941376\n",
      "Epoch 1/1, Step 210/248, Loss: 0.28768306970596313\n",
      "Epoch 1/1, Step 220/248, Loss: 0.21824657917022705\n",
      "Epoch 1/1, Step 230/248, Loss: 0.2653811275959015\n",
      "Epoch 1/1, Step 240/248, Loss: 0.263901948928833\n",
      "Epoch 1/1 completed. Average Loss: 0.3145156139207463\n",
      "Validation Loss: 0.2499628807344134\n"
     ]
    }
   ],
   "source": [
    "# Training loop with progress monitoring\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids, b_attention_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss_fct = BCEWithLogitsLoss()\n",
    "        loss = loss_fct(outputs.logits, b_labels.float())\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Step {step}/{len(train_dataloader)}, Loss: {loss.item()}\")\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed. Average Loss: {avg_train_loss}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "total_eval_loss = 0\n",
    "for batch in val_dataloader:\n",
    "    b_input_ids, b_attention_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss_fct = BCEWithLogitsLoss()\n",
    "        loss = loss_fct(outputs.logits, b_labels.float())\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "print(f\"Validation Loss: {avg_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f74694-99ba-4f40-969c-93fe9a2304c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2070528967254408\n",
      "Precision: 0.8800705467372134\n",
      "Recall: 0.20392317123007764\n",
      "F1 Score: 0.33112143331121435\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Initialize empty lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "for batch in val_dataloader:\n",
    "    b_input_ids, b_attention_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "        \n",
    "    # Store logits and true labels\n",
    "    logits = outputs.logits\n",
    "    all_predictions.append(logits.cpu().numpy())\n",
    "    all_true_labels.append(b_labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "predictions = np.concatenate(all_predictions, axis=0)\n",
    "true_labels = np.concatenate(all_true_labels, axis=0)\n",
    "\n",
    "# Convert logits to binary predictions\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='micro')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950276db-d994-49db-8931-9b1b35dc2581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
